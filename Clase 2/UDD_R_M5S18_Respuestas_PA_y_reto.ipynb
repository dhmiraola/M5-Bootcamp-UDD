{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ponte a prueba 1**\n",
        "\n",
        "Usa el código de la red neuronal anterior y:\n",
        "- Predice el género de tres personas.\n",
        "- Modifica el `learn_rate` y el número de repeticiones observando los resultados.\n",
        "- Entrena la red neuronal con otro *dataset*.\n",
        "- Grafica las pérdidas."
      ],
      "metadata": {
        "id": "5AdHBFgBirgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EUs0jDhc--hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiamos el código del notebook de la semana 18\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Función de activación sigmoide: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  # Derivada de sigmoide: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  # y_verdadera and y_verdadera son arrays con el nismo tamaño\n",
        "  return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "class MiRedNeuronal:\n",
        "  '''\n",
        "    Una red neuronal con:\n",
        "    - 2 entradas\n",
        "    - 1 capa oculta con 2 neuronas (h1, h2)\n",
        "    - Una capa de salida 1 neurona (o1)\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    # Pesos\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Sesgos\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x es un array de numpy de 2 elementos\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    '''\n",
        "    - La data es un array de numpy (n x 2) n =  n = # of muestras en el dataset\n",
        "    - todo_y_verdaderas es un array de numpy con n elementos que son la data\n",
        "    '''\n",
        "    learn_rate = 0.1\n",
        "    epochs = 1000 # número de veces para recorrer todo el conjunto de datos\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # Haz un feedforward (necesitaremos estos valores más adelante)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # Cálculo de las derivadas parciales.\n",
        "        # d_L_d_w1 representa \"parcial L / parcial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neurona o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neurona h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neurona h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # Actualización de pesos y sesgos\n",
        "        # Neurona h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neurona h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neurona o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # Calcular la pérdida total al final de cada epoch o época\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        print(\"Epoch %d perdida: %.3f\" % (epoch, loss))\n",
        "\n",
        "# Definimos el dataset\n",
        "data = np.array([\n",
        "  [-2, -1],  # Alicia\n",
        "  [25, 6],   # Juan\n",
        "  [17, 4],   # Carlos\n",
        "  [-15, -6], # Mari\n",
        "])\n",
        "all_y_trues = np.array([\n",
        "  1, # Alicia\n",
        "  0, # Juan\n",
        "  0, # Carlos\n",
        "  1, # Mari\n",
        "])\n",
        "\n",
        "# Entrenamos nuestra red neuronal\n",
        "network = MiRedNeuronal()\n",
        "network.train(data, all_y_trues)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TnavMwy3wE5",
        "outputId": "2e92ca8b-577e-4835-e946-a8f90902f2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 perdida: 0.234\n",
            "Epoch 10 perdida: 0.124\n",
            "Epoch 20 perdida: 0.086\n",
            "Epoch 30 perdida: 0.068\n",
            "Epoch 40 perdida: 0.055\n",
            "Epoch 50 perdida: 0.046\n",
            "Epoch 60 perdida: 0.039\n",
            "Epoch 70 perdida: 0.034\n",
            "Epoch 80 perdida: 0.029\n",
            "Epoch 90 perdida: 0.026\n",
            "Epoch 100 perdida: 0.023\n",
            "Epoch 110 perdida: 0.020\n",
            "Epoch 120 perdida: 0.018\n",
            "Epoch 130 perdida: 0.017\n",
            "Epoch 140 perdida: 0.015\n",
            "Epoch 150 perdida: 0.014\n",
            "Epoch 160 perdida: 0.013\n",
            "Epoch 170 perdida: 0.012\n",
            "Epoch 180 perdida: 0.011\n",
            "Epoch 190 perdida: 0.010\n",
            "Epoch 200 perdida: 0.010\n",
            "Epoch 210 perdida: 0.009\n",
            "Epoch 220 perdida: 0.009\n",
            "Epoch 230 perdida: 0.008\n",
            "Epoch 240 perdida: 0.008\n",
            "Epoch 250 perdida: 0.007\n",
            "Epoch 260 perdida: 0.007\n",
            "Epoch 270 perdida: 0.007\n",
            "Epoch 280 perdida: 0.006\n",
            "Epoch 290 perdida: 0.006\n",
            "Epoch 300 perdida: 0.006\n",
            "Epoch 310 perdida: 0.006\n",
            "Epoch 320 perdida: 0.005\n",
            "Epoch 330 perdida: 0.005\n",
            "Epoch 340 perdida: 0.005\n",
            "Epoch 350 perdida: 0.005\n",
            "Epoch 360 perdida: 0.005\n",
            "Epoch 370 perdida: 0.005\n",
            "Epoch 380 perdida: 0.004\n",
            "Epoch 390 perdida: 0.004\n",
            "Epoch 400 perdida: 0.004\n",
            "Epoch 410 perdida: 0.004\n",
            "Epoch 420 perdida: 0.004\n",
            "Epoch 430 perdida: 0.004\n",
            "Epoch 440 perdida: 0.004\n",
            "Epoch 450 perdida: 0.004\n",
            "Epoch 460 perdida: 0.004\n",
            "Epoch 470 perdida: 0.003\n",
            "Epoch 480 perdida: 0.003\n",
            "Epoch 490 perdida: 0.003\n",
            "Epoch 500 perdida: 0.003\n",
            "Epoch 510 perdida: 0.003\n",
            "Epoch 520 perdida: 0.003\n",
            "Epoch 530 perdida: 0.003\n",
            "Epoch 540 perdida: 0.003\n",
            "Epoch 550 perdida: 0.003\n",
            "Epoch 560 perdida: 0.003\n",
            "Epoch 570 perdida: 0.003\n",
            "Epoch 580 perdida: 0.003\n",
            "Epoch 590 perdida: 0.003\n",
            "Epoch 600 perdida: 0.003\n",
            "Epoch 610 perdida: 0.003\n",
            "Epoch 620 perdida: 0.003\n",
            "Epoch 630 perdida: 0.002\n",
            "Epoch 640 perdida: 0.002\n",
            "Epoch 650 perdida: 0.002\n",
            "Epoch 660 perdida: 0.002\n",
            "Epoch 670 perdida: 0.002\n",
            "Epoch 680 perdida: 0.002\n",
            "Epoch 690 perdida: 0.002\n",
            "Epoch 700 perdida: 0.002\n",
            "Epoch 710 perdida: 0.002\n",
            "Epoch 720 perdida: 0.002\n",
            "Epoch 730 perdida: 0.002\n",
            "Epoch 740 perdida: 0.002\n",
            "Epoch 750 perdida: 0.002\n",
            "Epoch 760 perdida: 0.002\n",
            "Epoch 770 perdida: 0.002\n",
            "Epoch 780 perdida: 0.002\n",
            "Epoch 790 perdida: 0.002\n",
            "Epoch 800 perdida: 0.002\n",
            "Epoch 810 perdida: 0.002\n",
            "Epoch 820 perdida: 0.002\n",
            "Epoch 830 perdida: 0.002\n",
            "Epoch 840 perdida: 0.002\n",
            "Epoch 850 perdida: 0.002\n",
            "Epoch 860 perdida: 0.002\n",
            "Epoch 870 perdida: 0.002\n",
            "Epoch 880 perdida: 0.002\n",
            "Epoch 890 perdida: 0.002\n",
            "Epoch 900 perdida: 0.002\n",
            "Epoch 910 perdida: 0.002\n",
            "Epoch 920 perdida: 0.002\n",
            "Epoch 930 perdida: 0.002\n",
            "Epoch 940 perdida: 0.002\n",
            "Epoch 950 perdida: 0.002\n",
            "Epoch 960 perdida: 0.002\n",
            "Epoch 970 perdida: 0.002\n",
            "Epoch 980 perdida: 0.002\n",
            "Epoch 990 perdida: 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Respuesta con comentarios para explicar el proceso"
      ],
      "metadata": {
        "id": "QeEeCF7Yiw_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Predicción del género de tres personas"
      ],
      "metadata": {
        "id": "CofyWk0_30_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos información para 3 nuevas personas\n",
        "Emma = np.array([-6, -4])\n",
        "Lucas = np.array([23, 5])\n",
        "Maria = np.array([-5, -4])\n",
        "\n",
        "print(\"Emma: %.3f\" % network.feedforward(Emma))\n",
        "print(\"Lucas: %.3f\" % network.feedforward(Lucas))\n",
        "print(\"Maria: %.3f\" % network.feedforward(Maria))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txi9LQgD346a",
        "outputId": "6ad996db-a5bc-4dc1-8da1-11848472e8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emma: 0.969\n",
            "Lucas: 0.039\n",
            "Maria: 0.969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modificación del learn_rate y número de repeticiones\n",
        "\n",
        "Aumentaremos el learn_rate a 0.5 y el número de repeticiones (epochs) a 5000 para observar si hay alguna mejora en los resultados.\n",
        "\n",
        "3. Entrenamiento con otro dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "6ykchF4N4ozQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiaremos el learn_rate, las epochs y el dataset\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Función de activación sigmoide: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  # Derivada de sigmoide: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  # y_verdadera and y_verdadera son arrays con el nismo tamaño\n",
        "  return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "class MiRedNeuronal:\n",
        "  '''\n",
        "    Una red neuronal con:\n",
        "    - 2 entradas\n",
        "    - 1 capa oculta con 2 neuronas (h1, h2)\n",
        "    - Una capa de salida 1 neurona (o1)\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    # Pesos\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Sesgos\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "    # Agregamos una variable para guardar las perdidas\n",
        "    self.losses = []\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x es un array de numpy de 2 elementos\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    '''\n",
        "    - La data es un array de numpy (n x 2) n =  n = # of muestras en el dataset\n",
        "    - todo_y_verdaderas es un array de numpy con n elementos que son la data\n",
        "    '''\n",
        "    learn_rate = 0.5\n",
        "    epochs = 5000 # número de veces para recorrer todo el conjunto de datos\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # Haz un feedforward (necesitaremos estos valores más adelante)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # Cálculo de las derivadas parciales.\n",
        "        # d_L_d_w1 representa \"parcial L / parcial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neurona o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neurona h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neurona h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # Actualización de pesos y sesgos\n",
        "        # Neurona h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neurona h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neurona o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # Guardamos las perdidas en la variable creada anteriormente\n",
        "      y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "      loss = mse_loss(all_y_trues, y_preds)\n",
        "      self.losses.append(loss)\n",
        "\n",
        "      # Calcular la pérdida total al final de cada epoch o época\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        print(\"Epoch %d perdida: %.3f\" % (epoch, loss))\n",
        "\n",
        "data = np.array([\n",
        "  [-4, -2],\n",
        "  [28, 10],\n",
        "  [20, 6],\n",
        "  [-10, -8],\n",
        "])\n",
        "all_y_trues = np.array([\n",
        "  1, # Mujer\n",
        "  0, # Hombre\n",
        "  0, # Hombre\n",
        "  1, # Mujer\n",
        "])\n",
        "\n",
        "# Entrenamos nuestra red neuronal\n",
        "network = MiRedNeuronal()\n",
        "network.train(data, all_y_trues)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqsrQpF96K1F",
        "outputId": "39e7bc06-5078-462c-a515-654c6b57c57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 perdida: 0.258\n",
            "Epoch 10 perdida: 0.049\n",
            "Epoch 20 perdida: 0.021\n",
            "Epoch 30 perdida: 0.013\n",
            "Epoch 40 perdida: 0.009\n",
            "Epoch 50 perdida: 0.007\n",
            "Epoch 60 perdida: 0.006\n",
            "Epoch 70 perdida: 0.005\n",
            "Epoch 80 perdida: 0.004\n",
            "Epoch 90 perdida: 0.004\n",
            "Epoch 100 perdida: 0.003\n",
            "Epoch 110 perdida: 0.003\n",
            "Epoch 120 perdida: 0.003\n",
            "Epoch 130 perdida: 0.002\n",
            "Epoch 140 perdida: 0.002\n",
            "Epoch 150 perdida: 0.002\n",
            "Epoch 160 perdida: 0.002\n",
            "Epoch 170 perdida: 0.002\n",
            "Epoch 180 perdida: 0.002\n",
            "Epoch 190 perdida: 0.002\n",
            "Epoch 200 perdida: 0.001\n",
            "Epoch 210 perdida: 0.001\n",
            "Epoch 220 perdida: 0.001\n",
            "Epoch 230 perdida: 0.001\n",
            "Epoch 240 perdida: 0.001\n",
            "Epoch 250 perdida: 0.001\n",
            "Epoch 260 perdida: 0.001\n",
            "Epoch 270 perdida: 0.001\n",
            "Epoch 280 perdida: 0.001\n",
            "Epoch 290 perdida: 0.001\n",
            "Epoch 300 perdida: 0.001\n",
            "Epoch 310 perdida: 0.001\n",
            "Epoch 320 perdida: 0.001\n",
            "Epoch 330 perdida: 0.001\n",
            "Epoch 340 perdida: 0.001\n",
            "Epoch 350 perdida: 0.001\n",
            "Epoch 360 perdida: 0.001\n",
            "Epoch 370 perdida: 0.001\n",
            "Epoch 380 perdida: 0.001\n",
            "Epoch 390 perdida: 0.001\n",
            "Epoch 400 perdida: 0.001\n",
            "Epoch 410 perdida: 0.001\n",
            "Epoch 420 perdida: 0.001\n",
            "Epoch 430 perdida: 0.001\n",
            "Epoch 440 perdida: 0.001\n",
            "Epoch 450 perdida: 0.001\n",
            "Epoch 460 perdida: 0.001\n",
            "Epoch 470 perdida: 0.001\n",
            "Epoch 480 perdida: 0.001\n",
            "Epoch 490 perdida: 0.001\n",
            "Epoch 500 perdida: 0.001\n",
            "Epoch 510 perdida: 0.001\n",
            "Epoch 520 perdida: 0.001\n",
            "Epoch 530 perdida: 0.001\n",
            "Epoch 540 perdida: 0.001\n",
            "Epoch 550 perdida: 0.000\n",
            "Epoch 560 perdida: 0.000\n",
            "Epoch 570 perdida: 0.000\n",
            "Epoch 580 perdida: 0.000\n",
            "Epoch 590 perdida: 0.000\n",
            "Epoch 600 perdida: 0.000\n",
            "Epoch 610 perdida: 0.000\n",
            "Epoch 620 perdida: 0.000\n",
            "Epoch 630 perdida: 0.000\n",
            "Epoch 640 perdida: 0.000\n",
            "Epoch 650 perdida: 0.000\n",
            "Epoch 660 perdida: 0.000\n",
            "Epoch 670 perdida: 0.000\n",
            "Epoch 680 perdida: 0.000\n",
            "Epoch 690 perdida: 0.000\n",
            "Epoch 700 perdida: 0.000\n",
            "Epoch 710 perdida: 0.000\n",
            "Epoch 720 perdida: 0.000\n",
            "Epoch 730 perdida: 0.000\n",
            "Epoch 740 perdida: 0.000\n",
            "Epoch 750 perdida: 0.000\n",
            "Epoch 760 perdida: 0.000\n",
            "Epoch 770 perdida: 0.000\n",
            "Epoch 780 perdida: 0.000\n",
            "Epoch 790 perdida: 0.000\n",
            "Epoch 800 perdida: 0.000\n",
            "Epoch 810 perdida: 0.000\n",
            "Epoch 820 perdida: 0.000\n",
            "Epoch 830 perdida: 0.000\n",
            "Epoch 840 perdida: 0.000\n",
            "Epoch 850 perdida: 0.000\n",
            "Epoch 860 perdida: 0.000\n",
            "Epoch 870 perdida: 0.000\n",
            "Epoch 880 perdida: 0.000\n",
            "Epoch 890 perdida: 0.000\n",
            "Epoch 900 perdida: 0.000\n",
            "Epoch 910 perdida: 0.000\n",
            "Epoch 920 perdida: 0.000\n",
            "Epoch 930 perdida: 0.000\n",
            "Epoch 940 perdida: 0.000\n",
            "Epoch 950 perdida: 0.000\n",
            "Epoch 960 perdida: 0.000\n",
            "Epoch 970 perdida: 0.000\n",
            "Epoch 980 perdida: 0.000\n",
            "Epoch 990 perdida: 0.000\n",
            "Epoch 1000 perdida: 0.000\n",
            "Epoch 1010 perdida: 0.000\n",
            "Epoch 1020 perdida: 0.000\n",
            "Epoch 1030 perdida: 0.000\n",
            "Epoch 1040 perdida: 0.000\n",
            "Epoch 1050 perdida: 0.000\n",
            "Epoch 1060 perdida: 0.000\n",
            "Epoch 1070 perdida: 0.000\n",
            "Epoch 1080 perdida: 0.000\n",
            "Epoch 1090 perdida: 0.000\n",
            "Epoch 1100 perdida: 0.000\n",
            "Epoch 1110 perdida: 0.000\n",
            "Epoch 1120 perdida: 0.000\n",
            "Epoch 1130 perdida: 0.000\n",
            "Epoch 1140 perdida: 0.000\n",
            "Epoch 1150 perdida: 0.000\n",
            "Epoch 1160 perdida: 0.000\n",
            "Epoch 1170 perdida: 0.000\n",
            "Epoch 1180 perdida: 0.000\n",
            "Epoch 1190 perdida: 0.000\n",
            "Epoch 1200 perdida: 0.000\n",
            "Epoch 1210 perdida: 0.000\n",
            "Epoch 1220 perdida: 0.000\n",
            "Epoch 1230 perdida: 0.000\n",
            "Epoch 1240 perdida: 0.000\n",
            "Epoch 1250 perdida: 0.000\n",
            "Epoch 1260 perdida: 0.000\n",
            "Epoch 1270 perdida: 0.000\n",
            "Epoch 1280 perdida: 0.000\n",
            "Epoch 1290 perdida: 0.000\n",
            "Epoch 1300 perdida: 0.000\n",
            "Epoch 1310 perdida: 0.000\n",
            "Epoch 1320 perdida: 0.000\n",
            "Epoch 1330 perdida: 0.000\n",
            "Epoch 1340 perdida: 0.000\n",
            "Epoch 1350 perdida: 0.000\n",
            "Epoch 1360 perdida: 0.000\n",
            "Epoch 1370 perdida: 0.000\n",
            "Epoch 1380 perdida: 0.000\n",
            "Epoch 1390 perdida: 0.000\n",
            "Epoch 1400 perdida: 0.000\n",
            "Epoch 1410 perdida: 0.000\n",
            "Epoch 1420 perdida: 0.000\n",
            "Epoch 1430 perdida: 0.000\n",
            "Epoch 1440 perdida: 0.000\n",
            "Epoch 1450 perdida: 0.000\n",
            "Epoch 1460 perdida: 0.000\n",
            "Epoch 1470 perdida: 0.000\n",
            "Epoch 1480 perdida: 0.000\n",
            "Epoch 1490 perdida: 0.000\n",
            "Epoch 1500 perdida: 0.000\n",
            "Epoch 1510 perdida: 0.000\n",
            "Epoch 1520 perdida: 0.000\n",
            "Epoch 1530 perdida: 0.000\n",
            "Epoch 1540 perdida: 0.000\n",
            "Epoch 1550 perdida: 0.000\n",
            "Epoch 1560 perdida: 0.000\n",
            "Epoch 1570 perdida: 0.000\n",
            "Epoch 1580 perdida: 0.000\n",
            "Epoch 1590 perdida: 0.000\n",
            "Epoch 1600 perdida: 0.000\n",
            "Epoch 1610 perdida: 0.000\n",
            "Epoch 1620 perdida: 0.000\n",
            "Epoch 1630 perdida: 0.000\n",
            "Epoch 1640 perdida: 0.000\n",
            "Epoch 1650 perdida: 0.000\n",
            "Epoch 1660 perdida: 0.000\n",
            "Epoch 1670 perdida: 0.000\n",
            "Epoch 1680 perdida: 0.000\n",
            "Epoch 1690 perdida: 0.000\n",
            "Epoch 1700 perdida: 0.000\n",
            "Epoch 1710 perdida: 0.000\n",
            "Epoch 1720 perdida: 0.000\n",
            "Epoch 1730 perdida: 0.000\n",
            "Epoch 1740 perdida: 0.000\n",
            "Epoch 1750 perdida: 0.000\n",
            "Epoch 1760 perdida: 0.000\n",
            "Epoch 1770 perdida: 0.000\n",
            "Epoch 1780 perdida: 0.000\n",
            "Epoch 1790 perdida: 0.000\n",
            "Epoch 1800 perdida: 0.000\n",
            "Epoch 1810 perdida: 0.000\n",
            "Epoch 1820 perdida: 0.000\n",
            "Epoch 1830 perdida: 0.000\n",
            "Epoch 1840 perdida: 0.000\n",
            "Epoch 1850 perdida: 0.000\n",
            "Epoch 1860 perdida: 0.000\n",
            "Epoch 1870 perdida: 0.000\n",
            "Epoch 1880 perdida: 0.000\n",
            "Epoch 1890 perdida: 0.000\n",
            "Epoch 1900 perdida: 0.000\n",
            "Epoch 1910 perdida: 0.000\n",
            "Epoch 1920 perdida: 0.000\n",
            "Epoch 1930 perdida: 0.000\n",
            "Epoch 1940 perdida: 0.000\n",
            "Epoch 1950 perdida: 0.000\n",
            "Epoch 1960 perdida: 0.000\n",
            "Epoch 1970 perdida: 0.000\n",
            "Epoch 1980 perdida: 0.000\n",
            "Epoch 1990 perdida: 0.000\n",
            "Epoch 2000 perdida: 0.000\n",
            "Epoch 2010 perdida: 0.000\n",
            "Epoch 2020 perdida: 0.000\n",
            "Epoch 2030 perdida: 0.000\n",
            "Epoch 2040 perdida: 0.000\n",
            "Epoch 2050 perdida: 0.000\n",
            "Epoch 2060 perdida: 0.000\n",
            "Epoch 2070 perdida: 0.000\n",
            "Epoch 2080 perdida: 0.000\n",
            "Epoch 2090 perdida: 0.000\n",
            "Epoch 2100 perdida: 0.000\n",
            "Epoch 2110 perdida: 0.000\n",
            "Epoch 2120 perdida: 0.000\n",
            "Epoch 2130 perdida: 0.000\n",
            "Epoch 2140 perdida: 0.000\n",
            "Epoch 2150 perdida: 0.000\n",
            "Epoch 2160 perdida: 0.000\n",
            "Epoch 2170 perdida: 0.000\n",
            "Epoch 2180 perdida: 0.000\n",
            "Epoch 2190 perdida: 0.000\n",
            "Epoch 2200 perdida: 0.000\n",
            "Epoch 2210 perdida: 0.000\n",
            "Epoch 2220 perdida: 0.000\n",
            "Epoch 2230 perdida: 0.000\n",
            "Epoch 2240 perdida: 0.000\n",
            "Epoch 2250 perdida: 0.000\n",
            "Epoch 2260 perdida: 0.000\n",
            "Epoch 2270 perdida: 0.000\n",
            "Epoch 2280 perdida: 0.000\n",
            "Epoch 2290 perdida: 0.000\n",
            "Epoch 2300 perdida: 0.000\n",
            "Epoch 2310 perdida: 0.000\n",
            "Epoch 2320 perdida: 0.000\n",
            "Epoch 2330 perdida: 0.000\n",
            "Epoch 2340 perdida: 0.000\n",
            "Epoch 2350 perdida: 0.000\n",
            "Epoch 2360 perdida: 0.000\n",
            "Epoch 2370 perdida: 0.000\n",
            "Epoch 2380 perdida: 0.000\n",
            "Epoch 2390 perdida: 0.000\n",
            "Epoch 2400 perdida: 0.000\n",
            "Epoch 2410 perdida: 0.000\n",
            "Epoch 2420 perdida: 0.000\n",
            "Epoch 2430 perdida: 0.000\n",
            "Epoch 2440 perdida: 0.000\n",
            "Epoch 2450 perdida: 0.000\n",
            "Epoch 2460 perdida: 0.000\n",
            "Epoch 2470 perdida: 0.000\n",
            "Epoch 2480 perdida: 0.000\n",
            "Epoch 2490 perdida: 0.000\n",
            "Epoch 2500 perdida: 0.000\n",
            "Epoch 2510 perdida: 0.000\n",
            "Epoch 2520 perdida: 0.000\n",
            "Epoch 2530 perdida: 0.000\n",
            "Epoch 2540 perdida: 0.000\n",
            "Epoch 2550 perdida: 0.000\n",
            "Epoch 2560 perdida: 0.000\n",
            "Epoch 2570 perdida: 0.000\n",
            "Epoch 2580 perdida: 0.000\n",
            "Epoch 2590 perdida: 0.000\n",
            "Epoch 2600 perdida: 0.000\n",
            "Epoch 2610 perdida: 0.000\n",
            "Epoch 2620 perdida: 0.000\n",
            "Epoch 2630 perdida: 0.000\n",
            "Epoch 2640 perdida: 0.000\n",
            "Epoch 2650 perdida: 0.000\n",
            "Epoch 2660 perdida: 0.000\n",
            "Epoch 2670 perdida: 0.000\n",
            "Epoch 2680 perdida: 0.000\n",
            "Epoch 2690 perdida: 0.000\n",
            "Epoch 2700 perdida: 0.000\n",
            "Epoch 2710 perdida: 0.000\n",
            "Epoch 2720 perdida: 0.000\n",
            "Epoch 2730 perdida: 0.000\n",
            "Epoch 2740 perdida: 0.000\n",
            "Epoch 2750 perdida: 0.000\n",
            "Epoch 2760 perdida: 0.000\n",
            "Epoch 2770 perdida: 0.000\n",
            "Epoch 2780 perdida: 0.000\n",
            "Epoch 2790 perdida: 0.000\n",
            "Epoch 2800 perdida: 0.000\n",
            "Epoch 2810 perdida: 0.000\n",
            "Epoch 2820 perdida: 0.000\n",
            "Epoch 2830 perdida: 0.000\n",
            "Epoch 2840 perdida: 0.000\n",
            "Epoch 2850 perdida: 0.000\n",
            "Epoch 2860 perdida: 0.000\n",
            "Epoch 2870 perdida: 0.000\n",
            "Epoch 2880 perdida: 0.000\n",
            "Epoch 2890 perdida: 0.000\n",
            "Epoch 2900 perdida: 0.000\n",
            "Epoch 2910 perdida: 0.000\n",
            "Epoch 2920 perdida: 0.000\n",
            "Epoch 2930 perdida: 0.000\n",
            "Epoch 2940 perdida: 0.000\n",
            "Epoch 2950 perdida: 0.000\n",
            "Epoch 2960 perdida: 0.000\n",
            "Epoch 2970 perdida: 0.000\n",
            "Epoch 2980 perdida: 0.000\n",
            "Epoch 2990 perdida: 0.000\n",
            "Epoch 3000 perdida: 0.000\n",
            "Epoch 3010 perdida: 0.000\n",
            "Epoch 3020 perdida: 0.000\n",
            "Epoch 3030 perdida: 0.000\n",
            "Epoch 3040 perdida: 0.000\n",
            "Epoch 3050 perdida: 0.000\n",
            "Epoch 3060 perdida: 0.000\n",
            "Epoch 3070 perdida: 0.000\n",
            "Epoch 3080 perdida: 0.000\n",
            "Epoch 3090 perdida: 0.000\n",
            "Epoch 3100 perdida: 0.000\n",
            "Epoch 3110 perdida: 0.000\n",
            "Epoch 3120 perdida: 0.000\n",
            "Epoch 3130 perdida: 0.000\n",
            "Epoch 3140 perdida: 0.000\n",
            "Epoch 3150 perdida: 0.000\n",
            "Epoch 3160 perdida: 0.000\n",
            "Epoch 3170 perdida: 0.000\n",
            "Epoch 3180 perdida: 0.000\n",
            "Epoch 3190 perdida: 0.000\n",
            "Epoch 3200 perdida: 0.000\n",
            "Epoch 3210 perdida: 0.000\n",
            "Epoch 3220 perdida: 0.000\n",
            "Epoch 3230 perdida: 0.000\n",
            "Epoch 3240 perdida: 0.000\n",
            "Epoch 3250 perdida: 0.000\n",
            "Epoch 3260 perdida: 0.000\n",
            "Epoch 3270 perdida: 0.000\n",
            "Epoch 3280 perdida: 0.000\n",
            "Epoch 3290 perdida: 0.000\n",
            "Epoch 3300 perdida: 0.000\n",
            "Epoch 3310 perdida: 0.000\n",
            "Epoch 3320 perdida: 0.000\n",
            "Epoch 3330 perdida: 0.000\n",
            "Epoch 3340 perdida: 0.000\n",
            "Epoch 3350 perdida: 0.000\n",
            "Epoch 3360 perdida: 0.000\n",
            "Epoch 3370 perdida: 0.000\n",
            "Epoch 3380 perdida: 0.000\n",
            "Epoch 3390 perdida: 0.000\n",
            "Epoch 3400 perdida: 0.000\n",
            "Epoch 3410 perdida: 0.000\n",
            "Epoch 3420 perdida: 0.000\n",
            "Epoch 3430 perdida: 0.000\n",
            "Epoch 3440 perdida: 0.000\n",
            "Epoch 3450 perdida: 0.000\n",
            "Epoch 3460 perdida: 0.000\n",
            "Epoch 3470 perdida: 0.000\n",
            "Epoch 3480 perdida: 0.000\n",
            "Epoch 3490 perdida: 0.000\n",
            "Epoch 3500 perdida: 0.000\n",
            "Epoch 3510 perdida: 0.000\n",
            "Epoch 3520 perdida: 0.000\n",
            "Epoch 3530 perdida: 0.000\n",
            "Epoch 3540 perdida: 0.000\n",
            "Epoch 3550 perdida: 0.000\n",
            "Epoch 3560 perdida: 0.000\n",
            "Epoch 3570 perdida: 0.000\n",
            "Epoch 3580 perdida: 0.000\n",
            "Epoch 3590 perdida: 0.000\n",
            "Epoch 3600 perdida: 0.000\n",
            "Epoch 3610 perdida: 0.000\n",
            "Epoch 3620 perdida: 0.000\n",
            "Epoch 3630 perdida: 0.000\n",
            "Epoch 3640 perdida: 0.000\n",
            "Epoch 3650 perdida: 0.000\n",
            "Epoch 3660 perdida: 0.000\n",
            "Epoch 3670 perdida: 0.000\n",
            "Epoch 3680 perdida: 0.000\n",
            "Epoch 3690 perdida: 0.000\n",
            "Epoch 3700 perdida: 0.000\n",
            "Epoch 3710 perdida: 0.000\n",
            "Epoch 3720 perdida: 0.000\n",
            "Epoch 3730 perdida: 0.000\n",
            "Epoch 3740 perdida: 0.000\n",
            "Epoch 3750 perdida: 0.000\n",
            "Epoch 3760 perdida: 0.000\n",
            "Epoch 3770 perdida: 0.000\n",
            "Epoch 3780 perdida: 0.000\n",
            "Epoch 3790 perdida: 0.000\n",
            "Epoch 3800 perdida: 0.000\n",
            "Epoch 3810 perdida: 0.000\n",
            "Epoch 3820 perdida: 0.000\n",
            "Epoch 3830 perdida: 0.000\n",
            "Epoch 3840 perdida: 0.000\n",
            "Epoch 3850 perdida: 0.000\n",
            "Epoch 3860 perdida: 0.000\n",
            "Epoch 3870 perdida: 0.000\n",
            "Epoch 3880 perdida: 0.000\n",
            "Epoch 3890 perdida: 0.000\n",
            "Epoch 3900 perdida: 0.000\n",
            "Epoch 3910 perdida: 0.000\n",
            "Epoch 3920 perdida: 0.000\n",
            "Epoch 3930 perdida: 0.000\n",
            "Epoch 3940 perdida: 0.000\n",
            "Epoch 3950 perdida: 0.000\n",
            "Epoch 3960 perdida: 0.000\n",
            "Epoch 3970 perdida: 0.000\n",
            "Epoch 3980 perdida: 0.000\n",
            "Epoch 3990 perdida: 0.000\n",
            "Epoch 4000 perdida: 0.000\n",
            "Epoch 4010 perdida: 0.000\n",
            "Epoch 4020 perdida: 0.000\n",
            "Epoch 4030 perdida: 0.000\n",
            "Epoch 4040 perdida: 0.000\n",
            "Epoch 4050 perdida: 0.000\n",
            "Epoch 4060 perdida: 0.000\n",
            "Epoch 4070 perdida: 0.000\n",
            "Epoch 4080 perdida: 0.000\n",
            "Epoch 4090 perdida: 0.000\n",
            "Epoch 4100 perdida: 0.000\n",
            "Epoch 4110 perdida: 0.000\n",
            "Epoch 4120 perdida: 0.000\n",
            "Epoch 4130 perdida: 0.000\n",
            "Epoch 4140 perdida: 0.000\n",
            "Epoch 4150 perdida: 0.000\n",
            "Epoch 4160 perdida: 0.000\n",
            "Epoch 4170 perdida: 0.000\n",
            "Epoch 4180 perdida: 0.000\n",
            "Epoch 4190 perdida: 0.000\n",
            "Epoch 4200 perdida: 0.000\n",
            "Epoch 4210 perdida: 0.000\n",
            "Epoch 4220 perdida: 0.000\n",
            "Epoch 4230 perdida: 0.000\n",
            "Epoch 4240 perdida: 0.000\n",
            "Epoch 4250 perdida: 0.000\n",
            "Epoch 4260 perdida: 0.000\n",
            "Epoch 4270 perdida: 0.000\n",
            "Epoch 4280 perdida: 0.000\n",
            "Epoch 4290 perdida: 0.000\n",
            "Epoch 4300 perdida: 0.000\n",
            "Epoch 4310 perdida: 0.000\n",
            "Epoch 4320 perdida: 0.000\n",
            "Epoch 4330 perdida: 0.000\n",
            "Epoch 4340 perdida: 0.000\n",
            "Epoch 4350 perdida: 0.000\n",
            "Epoch 4360 perdida: 0.000\n",
            "Epoch 4370 perdida: 0.000\n",
            "Epoch 4380 perdida: 0.000\n",
            "Epoch 4390 perdida: 0.000\n",
            "Epoch 4400 perdida: 0.000\n",
            "Epoch 4410 perdida: 0.000\n",
            "Epoch 4420 perdida: 0.000\n",
            "Epoch 4430 perdida: 0.000\n",
            "Epoch 4440 perdida: 0.000\n",
            "Epoch 4450 perdida: 0.000\n",
            "Epoch 4460 perdida: 0.000\n",
            "Epoch 4470 perdida: 0.000\n",
            "Epoch 4480 perdida: 0.000\n",
            "Epoch 4490 perdida: 0.000\n",
            "Epoch 4500 perdida: 0.000\n",
            "Epoch 4510 perdida: 0.000\n",
            "Epoch 4520 perdida: 0.000\n",
            "Epoch 4530 perdida: 0.000\n",
            "Epoch 4540 perdida: 0.000\n",
            "Epoch 4550 perdida: 0.000\n",
            "Epoch 4560 perdida: 0.000\n",
            "Epoch 4570 perdida: 0.000\n",
            "Epoch 4580 perdida: 0.000\n",
            "Epoch 4590 perdida: 0.000\n",
            "Epoch 4600 perdida: 0.000\n",
            "Epoch 4610 perdida: 0.000\n",
            "Epoch 4620 perdida: 0.000\n",
            "Epoch 4630 perdida: 0.000\n",
            "Epoch 4640 perdida: 0.000\n",
            "Epoch 4650 perdida: 0.000\n",
            "Epoch 4660 perdida: 0.000\n",
            "Epoch 4670 perdida: 0.000\n",
            "Epoch 4680 perdida: 0.000\n",
            "Epoch 4690 perdida: 0.000\n",
            "Epoch 4700 perdida: 0.000\n",
            "Epoch 4710 perdida: 0.000\n",
            "Epoch 4720 perdida: 0.000\n",
            "Epoch 4730 perdida: 0.000\n",
            "Epoch 4740 perdida: 0.000\n",
            "Epoch 4750 perdida: 0.000\n",
            "Epoch 4760 perdida: 0.000\n",
            "Epoch 4770 perdida: 0.000\n",
            "Epoch 4780 perdida: 0.000\n",
            "Epoch 4790 perdida: 0.000\n",
            "Epoch 4800 perdida: 0.000\n",
            "Epoch 4810 perdida: 0.000\n",
            "Epoch 4820 perdida: 0.000\n",
            "Epoch 4830 perdida: 0.000\n",
            "Epoch 4840 perdida: 0.000\n",
            "Epoch 4850 perdida: 0.000\n",
            "Epoch 4860 perdida: 0.000\n",
            "Epoch 4870 perdida: 0.000\n",
            "Epoch 4880 perdida: 0.000\n",
            "Epoch 4890 perdida: 0.000\n",
            "Epoch 4900 perdida: 0.000\n",
            "Epoch 4910 perdida: 0.000\n",
            "Epoch 4920 perdida: 0.000\n",
            "Epoch 4930 perdida: 0.000\n",
            "Epoch 4940 perdida: 0.000\n",
            "Epoch 4950 perdida: 0.000\n",
            "Epoch 4960 perdida: 0.000\n",
            "Epoch 4970 perdida: 0.000\n",
            "Epoch 4980 perdida: 0.000\n",
            "Epoch 4990 perdida: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Gráfica de las pérdidas"
      ],
      "metadata": {
        "id": "0DJpnRTx7P04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar las pérdidas\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(network.losses)\n",
        "plt.title(\"Pérdida durante el entrenamiento\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Pérdida (MSE)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "qrDbcOvG-xpD",
        "outputId": "522a39f7-e490-4857-b05d-e1e83c63c2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La gráfica muestra cómo la pérdida (medida en MSE) disminuye a medida que avanza el entrenamiento."
      ],
      "metadata": {
        "id": "Fc0r7HiG_4Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ponte a prueba 2**\n",
        "\n",
        "Usaremos el conjunto de datos de clasificación multiclase de vinos para demostrar cómo implementar un MLP para clasificación multiclase.\n",
        "\n",
        "El objetivo es predecir el origen de un vino basado en sus características químicas.\n",
        "\n",
        "**Nota:** Podemos tomar como referencia el ejercicio con el conjunto de datos Iris del notebook semana 18.\n",
        "\n",
        "**Código base**"
      ],
      "metadata": {
        "id": "Hmn0fGY8LqKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Cargamos el conjunto de datos de vinos\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target"
      ],
      "metadata": {
        "id": "dA7nQHDxGe-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Respuesta con comentarios para explicar el proceso"
      ],
      "metadata": {
        "id": "rLVpCrLwi_mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "from numpy import argmax\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Cargamos el conjunto de datos de vinos\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Normalizamos los datos para que tengan media 0 y desviación estándar 1\n",
        "scaler = StandardScaler().fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Determinamos el número de características de entrada\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "# Definimos el modelo\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
        "\n",
        "# Evaluamos el modelo\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Precisión Test: %.3f' % acc)\n",
        "\n",
        "# Hacemos una predicción\n",
        "row = [1.423, 1.71, 2.43, 15.6, 127, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065]\n",
        "row = scaler.transform([row])\n",
        "yhat = model.predict(row)\n",
        "print('Predicho: %s (class=%d)' % (yhat, argmax(yhat)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwq4QGQSFKfu",
        "outputId": "d6bc5d7e-cdf7-497b-e6b4-1013107a2403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(119, 13) (59, 13) (119,) (59,)\n",
            "Precisión Test: 0.949\n",
            "1/1 [==============================] - 0s 267ms/step\n",
            "Predicho: [[1.23259846e-17 1.00000000e+00 1.24858895e-25]] (class=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Tamaños de los conjuntos de datos**:\n",
        "   - `(119, 13)`: Esto indica que el conjunto de entrenamiento `X_train` tiene 119 muestras y 13 características.\n",
        "   - `(59, 13)`: El conjunto de prueba `X_test` tiene 59 muestras y 13 características.\n",
        "   - `(119,)`: `y_train` tiene 119 etiquetas, correspondientes a las 119 muestras del conjunto de entrenamiento.\n",
        "   - `(59,)`: `y_test` tiene 59 etiquetas, correspondientes a las 59 muestras del conjunto de prueba.\n",
        "\n",
        "2. **Precisión del modelo**:\n",
        "   - `Precisión Test: 0.983`: La precisión del modelo en el conjunto de prueba es del 98.3%. Esto significa que el modelo hizo predicciones correctas para el 98.3% de las muestras en el conjunto de prueba. Es un resultado muy bueno, indica que el modelo es bastante preciso en sus predicciones.\n",
        "\n",
        "3. **Predicción de una muestra**:\n",
        "   - `Predicho: [[5.3824878e-16 1.0000000e+00 1.3634807e-20]] (class=1)`: Aquí se muestra la predicción del modelo para una muestra específica. La salida es un vector de probabilidades para cada clase. Las probabilidades son esencialmente 0 para la clase 0 y clase 2, pero muy cercanas a 1 para la clase 1. Por lo tanto, el modelo predice con alta confianza que esta muestra pertenece a la clase 1 (como lo indica el `(class=1)` al final).\n",
        "\n",
        "   - Las probabilidades son:\n",
        "     - `5.3824878e-16`: Probabilidad para la clase 0. Es un número extremadamente cercano a 0.\n",
        "     - `1.0000000e+00`: Probabilidad para la clase 1. Esencialmente es 1, lo que significa casi certeza.\n",
        "     - `1.3634807e-20`: Probabilidad para la clase 2. Es un número aún más cercano a 0 que la probabilidad para la clase 0.\n",
        "\n",
        "En resumen, el modelo ha funcionado muy bien en el conjunto de datos de vinos, con una precisión de prueba del 98.3%. Además, para la muestra de entrada proporcionada, el modelo predice con alta confianza que pertenece a la clase 1."
      ],
      "metadata": {
        "id": "5RLvTjfJFupS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reto semanal\n",
        "\n",
        "Usa el *dataset* de casas en California con el código base a continuación.\n",
        "\n",
        "Realiza lo siguiente:\n",
        "- Un modelo de MLP de regresión con Keras (ya que son datos para regresión).\n",
        "- Genera tres predicciones con datos aleatorios.\n",
        "- Calcula el MSE y RMSE.\n",
        "- Analiza los resultados.\n"
      ],
      "metadata": {
        "id": "HrGC_FIMioad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Código base**"
      ],
      "metadata": {
        "id": "N4TBEPG1MmDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "# 1. Cargar el conjunto de datos\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target"
      ],
      "metadata": {
        "id": "KjN8x6PuMoW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp6A0r5dJf7t"
      },
      "outputs": [],
      "source": [
        "#Respuesta con comentarios para explicar el proceso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6TaZQOu8IoNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar el conjunto de datos\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target"
      ],
      "metadata": {
        "id": "tVhFEePVHX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocesar los datos\n",
        "# Normalizamos los datos para que tengan media 0 y desviación estándar 1\n",
        "scaler_X = StandardScaler().fit(X)\n",
        "X_normalized = scaler_X.transform(X)\n",
        "\n",
        "# Dividimos los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "awLQyDqzHmYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Crear el modelo MLP de regresión\n",
        "n_features = X_train.shape[1]\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "68AI1YttIx-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt7uYElML1z9",
        "outputId": "22f3a0b6-2a20-4f5a-ae84-4c9ae666b844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7febf42e71c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generar tres predicciones con datos aleatorios\n",
        "indices_aleatorios = np.random.choice(X_test.shape[0], 3, replace=False)\n",
        "samples = X_test[indices_aleatorios]\n",
        "predicciones = model.predict(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEIZcsX9L4kX",
        "outputId": "05e4a92a-5413-4cf7-e2c7-deac1400c7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 66ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Calcular el MSE y RMSE para el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"MSE: {mse}, RMSE: {rmse}\")\n",
        "for i, sample in enumerate(samples):\n",
        "    print(f\"Muestra {i+1}: {sample}, Predicción: {predicciones[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrkj7-npL7WX",
        "outputId": "12a6530c-d449-475c-ad13-1f4277277a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 0s 1ms/step\n",
            "MSE: 0.2879114293334343, RMSE: 0.536573787408064\n",
            "Muestra 1: [-0.36781763  1.06160074 -0.17339536 -0.20399932 -1.0327101  -0.03061556\n",
            "  1.05254828 -0.84367843], Predicción: [1.1318402]\n",
            "Muestra 2: [-0.9166719  -0.44810276 -0.76393501 -0.10689495  0.55237027 -0.12089691\n",
            " -1.34924882  1.21773144], Predicción: [1.308382]\n",
            "Muestra 3: [ 1.73221976  0.58485227  0.32240518 -0.26964851 -0.66977527 -0.01022653\n",
            " -0.68910576  0.76851379], Predicción: [4.4079266]\n"
          ]
        }
      ]
    }
  ]
}