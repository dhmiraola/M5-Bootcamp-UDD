{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b88992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Definición de nuestro dataset\n",
    "class CSVDataset(Dataset):\n",
    "    # Cargamos el dataset\n",
    "    def __init__(self, path):\n",
    "        # Cargamos el archivo CSV como un dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # Guardamos los valores de entrada y los de salida\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # Nos aseguramos de que la data de entrada sea flotante\n",
    "        self.X = self.X.astype('float32')\n",
    "        # Etiquetamos el objetivo de codificación y asegúrese de que los valores sean flotantes\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # Sacamos el número de filas en el dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # Obtenemos una fila en un índice\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # Obtenemos los índices para entrenar y probar filas\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # Determinamos los tamaños\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # Calculamos los splits\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# Preparamos el dataset\n",
    "def prepare_data(path):\n",
    "    # Cargamos el dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # Calculamos los splits\n",
    "    train, test = dataset.get_splits()\n",
    "    # Preparamos cargadores de datos\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# Definición del modelo\n",
    "class MLP(Module):\n",
    "    # Definimos los elementos del modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # Entrada a la primera capa oculta\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # Segunda capa oculta\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # Tercera capa oculta y salida\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # Entrada de alimentación hacia delante\n",
    "    def forward(self, X):\n",
    "        # Entrada a la primera capa oculta\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # Segunda capa oculta\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # Tercera capa oculta y salida\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# Entrenamos el modelo\n",
    "def train_model(train_dl, model):\n",
    "    # Definimos la optimización\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # Pasamos por las diferentes epocas o repeticiones\n",
    "    for epoch in range(100):\n",
    "        # Enumeramos los mini lotes\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # Borramos los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            # Calculamos la salida del modelo\n",
    "            yhat = model(inputs)\n",
    "            # Calculamos la pérdida\n",
    "            loss = criterion(yhat, targets)\n",
    "            loss.backward()\n",
    "            # Actualizamos los pesos del modelo\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluación del modelo\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # Evaluamos el modelo con el dataset de prueba\n",
    "        yhat = model(inputs)\n",
    "        # Regresamos un numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # Redondeamos a valores de clases\n",
    "        yhat = yhat.round()\n",
    "        # Guardamos los valores\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # Calculamos la precisión\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474e13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una predicción de clase para una fila de datos\n",
    "def predict(row, model):\n",
    "    # Convertimos la fila en datos\n",
    "    row = Tensor([row])\n",
    "    # Hacemos la predicción\n",
    "    yhat = model(row)\n",
    "    # Devolvemos un array de numpy\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1295daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 116\n",
      "Precisión: 0.940\n",
      "Predecido: 0.998 (class=1)\n"
     ]
    }
   ],
   "source": [
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# Definimos la red neuronal\n",
    "model = MLP(34)\n",
    "# Entrenamos el modelo\n",
    "train_model(train_dl, model)\n",
    "# Evaluamos el modelo\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Precisión: %.3f' % acc)\n",
    "# Hacemos una predicción\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predecido: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
